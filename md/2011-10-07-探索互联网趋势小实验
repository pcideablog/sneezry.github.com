
当初想做这个实验纯属蛋疼～那么就说说实验的思路和步骤吧。以百度新闻为入口，通过抓起网页，分析网页内容中的词语，然后对词语出现的次数进行统计，抓取网页内容的同时抓取中其中的超级链接并存入数据库，当此页内容分析完毕后就从数据库中获取下一个爬取的URL。思路很简单，实现也很方便，至于抓取URL的数量开始定为100，000条，后来想想这个数据量太大，估计服务器无法承受得住，于是减为10，000条，每次运行都会分析一个页面并搜集更多的URL，同时过滤掉重复记录的URL。对服务器设置计划任务，每两分钟运行一次，计划15天完成实验。之所以将任务间隔设置为两分钟是为了对计算资源消耗有所控制，如果资源消耗过快可以随时停止，防止任务运行过快来不及对计算资源的控制。昨天跑了一晚上，今天跑了一天，发现9000的资源积分已经消耗至6600，后台检测才刚刚抓取了436个URL，于是果断停止了这个实验。<br>由于实验样本比较小，与实际结果会产生一定的偏差，不过还是有一定的参考价值。在抓取的436个URL中，获取名词、特殊名词、人名、地名等名词性实词9，455个，其中排名前10的词与出现次数分别如下：<br>新闻(49081) 乔布斯(41111) 微博(30791) 视频(30107) 中国(21889) 新浪(21159) 首页(19288) 苹果(18305) 游戏(16464) 财经(15757)<br>完整报告请详见 <a href="http://sneezry.com/wp-content/uploads/sneezry.com/2011/10/results.html">http://sneezry.com/wp-content/uploads/sneezry.com/2011/10/results.html</a><br>实验环境：Sina App Engine，php，mysql

